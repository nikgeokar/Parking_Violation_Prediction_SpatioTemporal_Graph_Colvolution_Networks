{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Σε αυτό το Notebook πραγματοποιείται η εκπαίδευση του μοντέλου\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Proccesing\n",
    "import pandas as pd\n",
    "#Model\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense,add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "Project_Path='/Users/nickkarras/PycharmProjects/ParkingViolationPredictionGraph_Git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά φορτώνω το Dataset 'Legal_Illegal\", το αρχείο με τον καιρό \"Final_Weather_Data\", και το αρχείο με τις αποστάσεις των τομέων από τα 19 σημεία ενδιαφέροντος 'Distance_Data'. Χωρίζω το Dataset σε 80% train-set και 20% test-set και έπειτα τα σώζω σε ξεχωριστά αρχεία ώστε να έχω πάντα το ίδιο train-set και test-set για να ξέρω άν η κάθε αλλαγή βοηθάει. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_Data=pd.read_csv(Project_Path+ '/Data/Distance.csv',sep=',',index_col=0)\n",
    "Final_Weather_Data=pd.read_csv(Project_Path+ '/Data/Final_Weather_Data.csv',low_memory=False,sep=',',index_col=0)\n",
    "Legal_illegal=pd.read_csv(Project_Path+ '/Data/Scan_Data_Reg_2.3.csv',sep=',',index_col=0)\n",
    "\n",
    "Legal_illegal=Legal_illegal.loc[Legal_illegal['Key'] < '2020-01-01']\n",
    "Legal_illegal=Legal_illegal.dropna()\n",
    "Legal_illegal.to_csv(Project_Path+ '/Data/Full_TimeSeries.csv')\n",
    "\n",
    "train_data=Legal_illegal.loc[Legal_illegal['Key'] < '2019-10-01']\n",
    "test_data=Legal_illegal.loc[Legal_illegal['Key'] > '2019-10-01']\n",
    "\n",
    "train_data.to_csv(Project_Path+ '/Data/Train_TimeSeries.csv')\n",
    "test_data.to_csv(Project_Path+ '/Data/Test_TimeSeries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα παρακάτω κελιά αποτελούν συναρτήσεις που χρησιμοποιούνται σε όλο το αρχείο  κατά την διάρκεια της εκπαίδευσης "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτή η συνάρτηση δέχεται train-set και test-set. Εφαρμόζει σε αυτά standardization και τα επιστρέφει "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller(Train,Test):\n",
    "    Standar_Scaller = StandardScaler()\n",
    "    Scalled_Train_data=Standar_Scaller.fit_transform(Train)\n",
    "    Scalled_val_data = Standar_Scaller.transform(Test)\n",
    "    return Scalled_Train_data,Scalled_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 8:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.25)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δηλώνω Initializers για τους kernel των κρυφών στρωμάτων και τις εξόδου "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kernel_Sigmoid_Initializer = tf.keras.initializers.GlorotUniform()\n",
    "Kernel_Relu_Initializer =tf.keras.initializers.HeUniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτή η συνάρτηση κατασκευάζει το Residual Neural Networks. Δέχεται το μέγεθος των χαρακτηριστικών ωστε να το χρησιμοποιήσει σαν μέγεθος εισόδου. Δέχεται μια μεταβλητή που θα την χρησιμοποιήσει για να αρχικοποιήσει το bias του επιπέδου εξόδου "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Residual_NN(Input_Shape,Bias_Sigmoid_Initializer):\n",
    "    Input_Layer = Input(shape=(Input_Shape,))\n",
    "    #Dense_Layer1 = Dense(256, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Input_Layer)\n",
    "    Dense_Layer2 = Dense(128, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Input_Layer)\n",
    "    Dense_Layer3 = Dense(64, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer2)\n",
    "    Dense_Layer4 = Dense(32, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer3)\n",
    "    Dense_Layer5 = Dense(64, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer4)\n",
    "    Residual_Add = add([Dense_Layer3, Dense_Layer5])\n",
    "    Dense_Layer6 = Dense(16, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Residual_Add)\n",
    "    Output_Layer = Dense(1, activation='sigmoid',kernel_initializer=Kernel_Sigmoid_Initializer,bias_initializer=Bias_Sigmoid_Initializer)(Dense_Layer6)\n",
    "\n",
    "    \n",
    "    model2 = Model(inputs=Input_Layer, outputs=Output_Layer)\n",
    "    sgd=tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    model2.compile(optimizer='adamax', loss='mse', metrics='mae')\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Dataset(Dataset): \n",
    "    Dataset=Dataset.drop(['Time_Int'], axis=1)\n",
    "    a=Dataset['Slot_Timeint']\n",
    "    b=Dataset['Ilegality_Rate']\n",
    "    Dataset=Dataset.drop(['Slot_Timeint'], axis=1)\n",
    "    Dataset=Dataset.drop(['Ilegality_Rate'], axis=1)\n",
    "    Dataset.insert(8, \"Real_Time\", a, True)\n",
    "    Dataset.insert(9, \"Real_Rate\", b, True)\n",
    "    \n",
    "    Dataset=pd.merge(Dataset, Final_Weather_Data, on='Key')\n",
    "    Dataset=Dataset.drop(['Key'], axis=1)\n",
    "    \n",
    "    Dataset=pd.merge(Dataset, Distance_Data, on='Slot_id')\n",
    "    Dataset=Dataset.drop(['Slot_id'], axis=1)\n",
    "    return Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η παρακάτω συνάρτηση εφαρμόζει τεχνική εκθετικής μείωσης στο learning rate μετά την εποχή 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train_TimeSeries.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test_TimeSeries.csv',sep=',',index_col=0)\n",
    "\n",
    "\n",
    "#Καλεί την συνάρτηση ώστε να κάνει τις τελικές αλλάγες στο dataset ώστε να είναι έτοιμο για εκπαίδευση\n",
    "TestDF=Prepare_Dataset(test_data)\n",
    "TrainDF=Prepare_Dataset(train_data)\n",
    "\n",
    "#Χωρίζει σε στόχο και χαρακτηριστικά\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "test_targets=TestDF['Real_Rate']\n",
    "test_data=TestDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "\n",
    "#Βρίσκει το μέσο των στόχων και το χρησιμοποεί για να αρχικοποιήσει τα bias οταν δημιουργείται το μοντέλο \n",
    "Target=train_targets\n",
    "Bias_Initial_Out=Target.mean()\n",
    "Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "\n",
    "train_data,test_data=Scaller(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εκπαιδεύω για 10 εποχές"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5555/5555 [==============================] - 2s 373us/step - loss: 0.0594 - mae: 0.1951\n",
      "Epoch 2/10\n",
      "5555/5555 [==============================] - 2s 384us/step - loss: 0.0541 - mae: 0.1845\n",
      "Epoch 3/10\n",
      "5555/5555 [==============================] - 2s 402us/step - loss: 0.0518 - mae: 0.1795\n",
      "Epoch 4/10\n",
      "5555/5555 [==============================] - 2s 381us/step - loss: 0.0501 - mae: 0.1756\n",
      "Epoch 5/10\n",
      "5555/5555 [==============================] - 2s 372us/step - loss: 0.0487 - mae: 0.1728\n",
      "Epoch 6/10\n",
      "5555/5555 [==============================] - 2s 371us/step - loss: 0.0478 - mae: 0.1705\n",
      "Epoch 7/10\n",
      "5555/5555 [==============================] - 2s 371us/step - loss: 0.0468 - mae: 0.1687\n",
      "Epoch 8/10\n",
      "5555/5555 [==============================] - 2s 375us/step - loss: 0.0461 - mae: 0.1672\n",
      "Epoch 9/10\n",
      "5555/5555 [==============================] - 2s 384us/step - loss: 0.0455 - mae: 0.1657\n",
      "Epoch 10/10\n",
      "5555/5555 [==============================] - 2s 385us/step - loss: 0.0449 - mae: 0.1644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c9eb3d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Residual_NN(Input_Shape=train_data.shape[1],Bias_Sigmoid_Initializer=Bias_Initializer)\n",
    "model.fit(train_data,train_targets,epochs=10, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "model.save(Project_Path + '/DNN_Regressor')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.7",
   "language": "python",
   "name": "tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}