{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DNN Model</center></h1> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Proccesing\n",
    "import pandas as pd\n",
    "#Model\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense,add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "Project_Path='/Users/nickkarras/PycharmProjects/ParkingViolationPredictionGraph_Git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_Data=pd.read_csv(Project_Path+ '/Data/Distance.csv',sep=',',index_col=0)\n",
    "Final_Weather_Data=pd.read_csv(Project_Path+ '/Data/Final_Weather_Data.csv',low_memory=False,sep=',',index_col=0)\n",
    "Legal_illegal=pd.read_csv(Project_Path+ '/Data/Scan_Data_Reg_2.3.csv',sep=',',index_col=0)\n",
    "\n",
    "Legal_illegal=Legal_illegal.loc[Legal_illegal['Key'] < '2020-01-01']\n",
    "Legal_illegal=Legal_illegal.dropna()\n",
    "Legal_illegal.to_csv(Project_Path+ '/Data/Full_TimeSeries.csv')\n",
    "\n",
    "train_data=Legal_illegal.loc[Legal_illegal['Key'] < '2019-10-01']\n",
    "test_data=Legal_illegal.loc[Legal_illegal['Key'] > '2019-10-01']\n",
    "\n",
    "train_data.to_csv(Project_Path+ '/Data/Train_TimeSeries.csv')\n",
    "test_data.to_csv(Project_Path+ '/Data/Test_TimeSeries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller(Train,Test):\n",
    "    Standar_Scaller = StandardScaler()\n",
    "    Scalled_Train_data=Standar_Scaller.fit_transform(Train)\n",
    "    Scalled_val_data = Standar_Scaller.transform(Test)\n",
    "    return Scalled_Train_data,Scalled_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Dataset(Dataset): \n",
    "    Dataset=Dataset.drop(['Time_Int'], axis=1)\n",
    "    a=Dataset['Slot_Timeint']\n",
    "    b=Dataset['Ilegality_Rate']\n",
    "    Dataset=Dataset.drop(['Slot_Timeint'], axis=1)\n",
    "    Dataset=Dataset.drop(['Ilegality_Rate'], axis=1)\n",
    "    Dataset.insert(8, \"Real_Time\", a, True)\n",
    "    Dataset.insert(9, \"Real_Rate\", b, True)\n",
    "    \n",
    "    Dataset=pd.merge(Dataset, Final_Weather_Data, on='Key')\n",
    "    Dataset=Dataset.drop(['Key'], axis=1)\n",
    "    \n",
    "    Dataset=pd.merge(Dataset, Distance_Data, on='Slot_id')\n",
    "    Dataset=Dataset.drop(['Slot_id'], axis=1)\n",
    "    return Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(Project_Path+ '/Data/Train_TimeSeries.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test_TimeSeries.csv',sep=',',index_col=0)\n",
    "\n",
    "TestDF=Prepare_Dataset(test_data)\n",
    "TrainDF=Prepare_Dataset(train_data)\n",
    "\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "test_targets=TestDF['Real_Rate']\n",
    "test_data=TestDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "train_data,test_data=Scaller(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Residual_NN(Input_Shape,Bias_Sigmoid_Initializer,Kernel_Relu_Initializer,Kernel_Sigmoid_Initializer):\n",
    "    Input_Layer = Input(shape=(Input_Shape,))\n",
    "    Dense_Layer1 = Dense(256, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Input_Layer)\n",
    "    Dense_Layer2 = Dense(128, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer1)\n",
    "    Dense_Layer3 = Dense(64, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer2)\n",
    "    Dense_Layer4 = Dense(32, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer3)\n",
    "    Dense_Layer5 = Dense(64, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Dense_Layer4)\n",
    "    Residual_Add = add([Dense_Layer3, Dense_Layer5])\n",
    "    Dense_Layer6 = Dense(16, activation='relu',kernel_initializer=Kernel_Relu_Initializer)(Residual_Add)\n",
    "    Output_Layer = Dense(1, activation='sigmoid',kernel_initializer=Kernel_Sigmoid_Initializer,bias_initializer=Bias_Sigmoid_Initializer)(Dense_Layer6)\n",
    "    \n",
    "    model2 = Model(inputs=Input_Layer, outputs=Output_Layer)\n",
    "    model2.compile(optimizer='adamax', loss='mse', metrics='mae')\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 8:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.25)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias_Initial_Out=train_targets.mean()\n",
    "Bias_Initializer=tf.keras.initializers.Constant(Bias_Initial_Out)\n",
    "Kernel_Sigmoid_Initializer = tf.keras.initializers.GlorotUniform()\n",
    "Kernel_Relu_Initializer =tf.keras.initializers.HeUniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εκπαιδεύω για 10 εποχές"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Residual_NN(train_data.shape[1],Bias_Initializer,Kernel_Relu_Initializer,Kernel_Sigmoid_Initializer)\n",
    "model.fit(train_data,train_targets,epochs=10, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "model.save(Project_Path + '/DNN_Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.7",
   "language": "python",
   "name": "tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
