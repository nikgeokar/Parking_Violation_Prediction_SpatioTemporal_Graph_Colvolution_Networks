{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Smoothing with Label Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages enviroment and project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "Project_Path='Project/Path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Names=['Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','temp','humidity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι','Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names2=['Slot_id','Key','Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','Real_Rate','temp','humidity']\n",
    "Names3=['Slot_id','Capacity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names4=['Capacity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names5=['Slot_id','Covid','Holidays','Capacity','temp','humidity','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names6=['Slot_id','Key','Date_Sin','Covid','Holidays','Capacity2','Week_Day_Sin','Month_Sin','Real_Time','Real_Rate','temp','humidity']\n",
    "Names7=['Slot_id','Covid','Holidays','Capacity','temp','humidity','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Predictions','Target']\n",
    "Names9=['Slot_id','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Predictions']\n",
    "Names_LP=['Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','temp','humidity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι','Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Real_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data For Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Initial_Data(Data,Final_Weather_Data,Distance_Data): \n",
    "    Data['Time_Distance']=0\n",
    "    Data=Data.drop(['Time_Int'], axis=1)\n",
    "    a=Data['Slot_Timeint']\n",
    "    b=Data['Ilegality_Rate']\n",
    "    Data=Data.drop(['Slot_Timeint'], axis=1)\n",
    "    Data=Data.drop(['Ilegality_Rate'], axis=1)\n",
    "    Data.insert(9, \"Real_Time\", a, True)\n",
    "    Data.insert(10, \"Real_Rate\", b, True) \n",
    "    Data=pd.merge(Data, Final_Weather_Data, on='Key')\n",
    "    Data=pd.merge(Data, Distance_Data, on='Slot_id')\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Dataset(Data,Final_Weather_Data,Distance_Data): \n",
    "    Data=Data.drop(['Time_Int'], axis=1)\n",
    "    a=Data['Slot_Timeint']\n",
    "    b=Data['Ilegality_Rate']\n",
    "    Data=Data.drop(['Slot_Timeint'], axis=1)\n",
    "    Data=Data.drop(['Ilegality_Rate'], axis=1)\n",
    "    Data.insert(8, \"Real_Time\", a, True)\n",
    "    Data.insert(9, \"Real_Rate\", b, True)       \n",
    "    Data=pd.merge(Test, Final_Weather_Data, on='Key')\n",
    "    Data=pd.merge(Data, Distance_Df, on='Slot_id')\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalling X Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller_For_Prediction(Data):\n",
    "    Slot_id=Data['Slot_id']\n",
    "    Key=Data['Key']\n",
    "    Real_Rate=Data['Real_Rate']\n",
    "    \n",
    "    Data=Data.drop(['Slot_id'], axis=1)\n",
    "    Data=Data.drop(['Key'], axis=1)\n",
    "    Data=Data.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Standar_Scaller = pickle.load(open(Project_Path+ '/Standar_Scaller2.pkl','rb'))\n",
    "    Data = Standar_Scaller.transform(Data)\n",
    "    Data=pd.DataFrame(Data,columns =Names)\n",
    "    \n",
    "    Data.insert(0, \"Slot_id\", Slot_id, True)\n",
    "    Data.insert(1, \"Key\", Key, True)\n",
    "    Data.insert(10, \"Real_Rate\", Real_Rate, True)\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller_For_Graph(Train,Test):\n",
    "    Slot_id_Train=Train['Slot_id']\n",
    "    Key_Train=Train['Key']\n",
    "    Real_Rate_Train=Train['Real_Rate']\n",
    "    \n",
    "    Train=Train.drop(['Slot_id'], axis=1)\n",
    "    Train=Train.drop(['Key'], axis=1)\n",
    "    Train=Train.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Slot_id_Test=Test['Slot_id']\n",
    "    Key_Test=Test['Key']\n",
    "    Real_Rate_Test=Test['Real_Rate']\n",
    "    \n",
    "    Test=Test.drop(['Slot_id'], axis=1)\n",
    "    Test=Test.drop(['Key'], axis=1)\n",
    "    Test=Test.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Standar_Scaller = StandardScaler()\n",
    "    Train=Standar_Scaller.fit_transform(Train)\n",
    "    Test = Standar_Scaller.transform(Test)\n",
    "    \n",
    "    Train=pd.DataFrame(Train,columns = Names)\n",
    "    Test=pd.DataFrame(Test,columns = Names)\n",
    "\n",
    "    Train.insert(0, \"Slot_id\", Slot_id_Train, True)\n",
    "    Train.insert(1, \"Key\", Key_Train, True)\n",
    "    Train.insert(10, \"Real_Rate\", Real_Rate_Train, True)\n",
    "    \n",
    "    Test.insert(0, \"Slot_id\", Slot_id_Test, True)\n",
    "    Test.insert(1, \"Key\", Key_Test, True)\n",
    "    Test.insert(10, \"Real_Rate\", Real_Rate_Test, True)\n",
    "    \n",
    "    return Train,Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scalled_Distances(Data):\n",
    "    Distance_Data=Data\n",
    "    Distance_Data=pd.DataFrame(Distance_Data)\n",
    "    Distance_Data=Distance_Data[Names3]\n",
    "    Distance_Data.drop_duplicates(subset =\"Slot_id\",keep ='first', inplace = True)\n",
    "    Distance_Data=Distance_Data.reset_index()\n",
    "    Distance_Data=Distance_Data.drop(['index'], axis=1)\n",
    "    return Distance_Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graphs from initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Unique_Time_Id(Data):\n",
    "    Data=Data.sort_values(by=['Key'])\n",
    "    Time_Id=Data['Key'].unique()\n",
    "    return Time_Id   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delete_Slot_Info(Data):\n",
    "    Data=Data[Names2]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Subsets_and_Lenght(Data,Time_Id):\n",
    "    Subsets=[]\n",
    "    Subsets=[0 for i in range(len(Time_Id))]\n",
    "    Length=[]\n",
    "    Length=[0 for i in range(len(Time_Id))]\n",
    "    Mask=[0 for i in range(len(Time_Id))]\n",
    "\n",
    "    for i in tqdm( range (0,len(Time_Id))):\n",
    "        Subsets[i] = Data[Data[\"Key\"]==Time_Id[i]]\n",
    "        Length[i] = len(Subsets[i])\n",
    "        Mask[i] = Subsets[i]['Slot_id'].values.tolist()\n",
    "    return Subsets,Length,Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan):\n",
    "    Sectors=Legal_Ilegan['Slot_id'].unique()\n",
    "    Sectors=sorted(Sectors)\n",
    "    Datasets=[]\n",
    "    Datasets = [0 for i in range(len(Time_Id))]\n",
    "\n",
    "    for i in tqdm(range (0,len(Subsets))):\n",
    "        S=[]\n",
    "        List=[]\n",
    "        Train_List=Subsets[i].values.tolist()\n",
    "        for j in range (0,Length[i]):\n",
    "            S.append(Train_List[j][0])\n",
    "            List.append(Train_List[j])\n",
    "\n",
    "        for k in range (0,len(Sectors)):\n",
    "            List2=[]\n",
    "            if Sectors[k] not in S:\n",
    "                List2.append(int(Sectors[k]))\n",
    "\n",
    "                for l in range (1,12):\n",
    "                    List2.append(Train_List[0][l])\n",
    "                List.append(List2)\n",
    "\n",
    "        Datasets[i]=pd.DataFrame(List, columns=[Names6])\n",
    "        Datasets[i].Slot_id = Datasets[i].Slot_id.astype(int)\n",
    "    return Datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Distances(Datasets,Time_Id,Distance_Data):\n",
    "    Distance_Data_List=Distance_Data.values.tolist()\n",
    "    for i in tqdm(range(0,len(Time_Id))):\n",
    "        Dataset_List=Datasets[i].values.tolist()\n",
    "        Distances=[]\n",
    "        Distances_Df=[]\n",
    "        for j in range (0,len(Dataset_List)):\n",
    "            for k in range(0,len(Distance_Data_List)):\n",
    "                if Dataset_List[j][0]==Distance_Data_List[k][0]:\n",
    "                    Distances.append(Distance_Data_List[k][1:])\n",
    "                    break\n",
    "        Distances_Df=pd.DataFrame(Distances, columns=[Names4])\n",
    "        Datasets[i] = pd.concat([Datasets[i], Distances_Df], axis=1)\n",
    "\n",
    "    for i in tqdm(range (0,len(Time_Id) )):\n",
    "        Datasets[i]=Datasets[i].drop(['Capacity2'], axis=1)\n",
    "        Datasets[i].insert(5, 'Capacity', Datasets[i].pop('Capacity'))\n",
    "\n",
    "    return Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Predictions(Datasets,Time_Id,Length):\n",
    "\n",
    "    Predictions_Df=[]\n",
    "    Predictions_Df=[0 for i in range(len(Datasets))] \n",
    "    Real_Target=[]\n",
    "    Real_Target=[0 for i in range(len(Datasets))] \n",
    "\n",
    "    for i in tqdm(range (0,len(Time_Id))):\n",
    "        Real_Target[i]=np.array(Datasets[i]['Real_Rate'])\n",
    "\n",
    "        Feautures = Datasets[i][Names]\n",
    "        Test=np.array(Feautures)   \n",
    "        Predictions=model.predict([Test])\n",
    "        Predictions_Df[i]=Predictions\n",
    "\n",
    "    for i in range (0,len(Time_Id)):\n",
    "        for j in range (Length[i],222):\n",
    "            Real_Target[i][j]=Predictions_Df[i][j]\n",
    "    return Real_Target,Predictions_Df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Final_Dataset(Datasets,Real_Target,Predictions_Df,Time_Id):\n",
    "    Final_Dataset=[]\n",
    "    Final_Dataset=[0 for i in range(len(Time_Id))] \n",
    "    for i in tqdm(range (0,len(Time_Id))):\n",
    "        Predictions_Df[i]=pd.DataFrame(Predictions_Df[i],columns=['Predictions'])\n",
    "        Real_Target[i]=pd.DataFrame(Real_Target[i],columns=['Target'])\n",
    "        Final_Dataset[i]=pd.concat([Datasets[i][Names5],Predictions_Df[i],Real_Target[i]], axis=1)\n",
    "        Final_Dataset[i].columns=Names7\n",
    "    return Final_Dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Data_For_Predictions(test_data,Distance_Data,Legal_Ilegan):\n",
    "    Time_Id=Get_Unique_Time_Id(test_data)\n",
    "    test_data=Delete_Slot_Info(test_data)\n",
    "    Subsets,Length,Mask=Get_Subsets_and_Lenght(test_data,Time_Id)\n",
    "    Datasets=Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan)\n",
    "    Datasets=Get_Distances(Datasets,Time_Id,Distance_Data)\n",
    "    Real_Target,Predictions_Df=Make_Predictions(Datasets,Time_Id,Length)\n",
    "    return Real_Target,Predictions_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Data(train_data,Distance_Data,Legal_Ilegan,Real_Target,Predictions_Df):\n",
    "    Time_Id=Get_Unique_Time_Id(train_data)\n",
    "    train_data=Delete_Slot_Info(train_data)\n",
    "    Subsets,Length,Mask=Get_Subsets_and_Lenght(train_data,Time_Id)\n",
    "    Datasets=Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan)\n",
    "    Datasets=Get_Distances(Datasets,Time_Id,Distance_Data)    \n",
    "    Train_Dataset=Get_Final_Dataset(Datasets,Real_Target,Predictions_Df,Time_Id)\n",
    "    return Train_Dataset,Length,Mask,Time_Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Signal Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and initial preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_Data=pd.read_csv(Project_Path+ '/Data/Distance.csv',sep=',',index_col=0)\n",
    "Final_Weather_Data=pd.read_csv(Project_Path+ '/Data/Final_Weather_Data.csv',low_memory=False,sep=',',index_col=0)\n",
    "train_data=pd.read_csv(Project_Path+ '/Data/Train_TimeSeries.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test_TimeSeries.csv',sep=',',index_col=0)\n",
    "\n",
    "TestDF=Prepare_Initial_Data(test_data,Final_Weather_Data,Distance_Data) #No smoothing\n",
    "TrainDF=Prepare_Initial_Data(train_data,Final_Weather_Data,Distance_Data)\n",
    "\n",
    "TrainDF=TrainDF.drop(['Time_Distance'], axis=1)\n",
    "TestDF=TestDF.drop(['Time_Distance'], axis=1)\n",
    "\n",
    "TrainDF.to_csv(Project_Path+ '/Data/Raw_Train_TimeSeries.csv')\n",
    "TestDF.to_csv(Project_Path+ '/Data/Raw_Test_TimeSeries.csv')\n",
    "\n",
    "TrainDF=TrainDF.drop(['Slot_id'], axis=1)\n",
    "TrainDF=TrainDF.drop(['Key'], axis=1)\n",
    "\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "Standar_Scaller = StandardScaler()\n",
    "train_data=Standar_Scaller.fit_transform(train_data)\n",
    "\n",
    "with open(Project_Path+'/Standar_Scaller2.pkl', 'wb') as f:\n",
    "    pickle.dump(Standar_Scaller, f,  protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(Project_Path+ '/DNN_Regressor')\n",
    "model.compile(optimizer='adamax', loss='MSE', metrics=['MAE'])\n",
    "\n",
    "Legal_Ilegan=pd.read_csv(Project_Path+ '/Data/Full_TimeSeries.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Raw_Test_TimeSeries.csv',sep=',',index_col=0)\n",
    "train_data=pd.read_csv(Project_Path+ '/Data/Raw_Train_TimeSeries.csv',sep=',',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Legal_Ilegan=Prepare_Dataset(Legal_Ilegan,Final_Weather_Data,Distance_Data)\n",
    "test_data1=Scaller_For_Prediction(test_data)\n",
    "train_data1=Scaller_For_Prediction(train_data)\n",
    "Legal_Ilegan1=Scaller_For_Prediction(Legal_Ilegan)\n",
    "Distance_Data_For_Predictions=Scalled_Distances(Legal_Ilegan1)\n",
    "train_data,test_data=Scaller_For_Graph(train_data,test_data)\n",
    "Distance_Data=Scalled_Distances(Legal_Ilegan1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real_Target_Train,Predictions_Df_Train=Prepare_Data_For_Predictions(train_data1,Distance_Data_For_Predictions,Legal_Ilegan)\n",
    "Train_Dataset,Length_Train,Train_Mask,Train_Date=Prepare_Data(train_data,Distance_Data,Legal_Ilegan,Real_Target_Train,Predictions_Df_Train)\n",
    "\n",
    "save_object(Train_Dataset, Project_Path+'/Data/Dataset_Graph_For_LP.pkl')\n",
    "with open(Project_Path+'/Data/Length_RawTrain.txt', \"w\") as file:\n",
    "    file.write(str(Length_Train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Slots=Train_Dataset[0]['Slot_id']\n",
    "Slots=sorted(Slots)\n",
    "\n",
    "Train_Binary_Mask=[]\n",
    "for j in range(0,len(Train_Mask)):\n",
    "    a=[]\n",
    "    for i in range (0,len(Slots)):\n",
    "        a.append(Slots[i] in (Train_Mask[j]))\n",
    "    Train_Binary_Mask.append(a)\n",
    "\n",
    "Train_Binary_Mask=pd.DataFrame(Train_Binary_Mask,columns=Slots,index=Train_Date)\n",
    "Train_Binary_Mask=Train_Binary_Mask.T\n",
    "Train_Binary_Mask = Train_Binary_Mask.replace(False, 0)\n",
    "Train_Binary_Mask = Train_Binary_Mask.replace(True, 1)\n",
    "Train_Binary_Mask.to_csv(Project_Path+ '/Data/Raw_Train_Mask.csv')\n",
    "Train_Binary_Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Propagation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sectors=pd.read_csv(Project_Path+ '/Data/Parking_Slot_Proccesed.csv',sep=',', index_col=0)\n",
    "Nodelist=Sectors[['Slot_id','Latitude','Longitude']]\n",
    "Legal_Ilegan2=pd.read_csv(Project_Path+ '/Data/Scan_Data_Reg_2.3.csv',sep=',',index_col=0)\n",
    "Unique_Sectors=Legal_Ilegan2['Slot_id'].unique()\n",
    "Unique_Sectors=pd.DataFrame(Unique_Sectors, columns=['Slot_id'])\n",
    "Nodelist=pd.merge(Unique_Sectors,Nodelist, on='Slot_id')\n",
    "Nodelist['Slot_id']=sorted(Nodelist['Slot_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find nearest unlabeled nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List=Nodelist.values.tolist()\n",
    "Distance=[]\n",
    "for i in range (0,len(List)):\n",
    "    Distance2=[]\n",
    "    for j in range (0,len(List)):\n",
    "        d= geopy.distance.geodesic((List[i][1],List[i][2]), (List[j][1],List[j][2])).m\n",
    "        Distance2.append(d)\n",
    "    Distance.append(Distance2)\n",
    "Names=Nodelist['Slot_id'].values.tolist()\n",
    "Distance=pd.DataFrame(Distance, columns=Names, index=Names)\n",
    "\n",
    "Columns=Distance.columns\n",
    "NearSectors=[]\n",
    "for i in tqdm(range(0,len(Columns))):\n",
    "    Nearest=Distance.nsmallest(3,Columns[i] )\n",
    "    Nearest=Nearest[Columns[i]]\n",
    "    Nearest=Nearest.reset_index(level=0)\n",
    "    Nearest=Nearest.values.tolist()\n",
    "    NearSectors.append(Nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Target_Sectros(Train_Dataset,NearSectors):\n",
    "    Sectors_List=[]\n",
    "    for k in tqdm(range (0,len(Train_Dataset))):\n",
    "        Existing_Sectors=Train_Dataset[k]['Slot_id'].values.tolist()\n",
    "        ls=[]\n",
    "        for i in range (0,Length_Train[k]):\n",
    "            ls.append(Existing_Sectors[i])\n",
    "            for j in range (0,len(NearSectors)):\n",
    "                if Existing_Sectors[i]==int(NearSectors[j][0][0]): \n",
    "                    if int(NearSectors[j][1][0]) not in Existing_Sectors[0:Length_Train[k]]:\n",
    "                        ls.append(int(NearSectors[j][1][0]))\n",
    "                    if NearSectors[j][2][0] not in Existing_Sectors[0:Length_Train[k]]:\n",
    "                        ls.append(int(NearSectors[j][2][0]))\n",
    "        x= np.array(ls)\n",
    "        ls=np.unique(x)       \n",
    "        Sectors_List.append(ls)\n",
    "    return Sectors_List        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sectors_List=Get_Target_Sectros(Train_Dataset,NearSectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range (0,len(Train_Dataset))):\n",
    "    T=[]\n",
    "    ls=Train_Dataset[i]['Slot_id'].values.tolist()\n",
    "    for j in range (0,len(ls)):\n",
    "        if ls[j] not in Sectors_List[i]:\n",
    "            T.append(False)\n",
    "        else:\n",
    "            T.append(True)\n",
    "    Train_Dataset[i]['Flag']=T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(Train_Dataset, Project_Path+ '/Data/Dataset_Graph_For_LP.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Label Propagation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+'/Data/Dataset_Graph_For_LP.pkl', 'rb') as inp:\n",
    "    Train_Dataset = pickle.load(inp)\n",
    "\n",
    "for i in range (0,len(Train_Dataset)):\n",
    "    Train_Dataset[i]['Target'].mask(Train_Dataset[i]['Target'] == Train_Dataset[i]['Predictions'], -1, inplace=True)\n",
    "    Train_Dataset[i]=Train_Dataset[i].loc[Train_Dataset[i]['Flag'] == True]\n",
    "    \n",
    "Dataset = pd.concat(Train_Dataset, axis=0)\n",
    "Dataset['Real_Rate']=Dataset['Target']\n",
    "TrainDF=Dataset[Names_LP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert regression to classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDF['Real_Rate']=round(TrainDF['Real_Rate'],2)\n",
    "TrainDF['Real_Rate']=TrainDF['Real_Rate']*100\n",
    "TrainDF['Real_Rate']=TrainDF['Real_Rate'].astype(int)\n",
    "TrainDF['Real_Rate'].mask(TrainDF['Real_Rate'] == -100, -1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=TrainDF\n",
    "y=Data['Real_Rate']\n",
    "X=Data.drop(['Real_Rate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rates=Data['Real_Rate'].unique()\n",
    "Rates=Rates/100\n",
    "Rates=sorted(Rates)\n",
    "Rates=Rates[1:]\n",
    "len(Rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LabelPropagation(kernel='knn',n_neighbors=301, max_iter=1000,n_jobs=-1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Project_Path +'/LabelPropagation.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.7",
   "language": "python",
   "name": "tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
