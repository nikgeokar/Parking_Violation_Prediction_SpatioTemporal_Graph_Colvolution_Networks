{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Creation from Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages Enviroment and Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import geopy.geocoders\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopy.distance\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "Project_Path='/Users/nickkarras/PycharmProjects/ParkingViolationPredictionGraph_Git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Names=['Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','temp','humidity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι','Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names2=['Slot_id','Key','Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','Real_Rate','temp','humidity']\n",
    "Names3=['Slot_id','Capacity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names4=['Capacity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names5=['Slot_id','Covid','Holidays','Capacity','temp','humidity','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names6=['Slot_id','Key','Date_Sin','Covid','Holidays','Capacity2','Week_Day_Sin','Month_Sin','Real_Time','Real_Rate','temp','humidity']\n",
    "Names7=['Slot_id','Covid','Holidays','Capacity','temp','humidity','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Predictions','Target']\n",
    "Names9=['Slot_id','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Predictions']\n",
    "Names_LP=['Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','temp','humidity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι','Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Real_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data For Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standar Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Dataset(Test): \n",
    "    Test=Test.drop(['Time_Int'], axis=1)\n",
    "    a=Test['Slot_Timeint']\n",
    "    b=Test['Ilegality_Rate']\n",
    "    Test=Test.drop(['Slot_Timeint'], axis=1)\n",
    "    Test=Test.drop(['Ilegality_Rate'], axis=1)\n",
    "    Test.insert(8, \"Real_Time\", a, True)\n",
    "    Test.insert(9, \"Real_Rate\", b, True)\n",
    "    \n",
    "    Final_Weather_Data=pd.read_csv(Project_Path+ '/Data/Final_Weather_Data.csv',low_memory=False,sep=',',index_col=0)\n",
    "    Test=pd.merge(Test, Final_Weather_Data, on='Key')\n",
    "    Distance_Df=pd.read_csv(Project_Path+ '/Data/Distance.csv',sep=',',index_col=0)\n",
    "    Test=pd.merge(Test, Distance_Df, on='Slot_id')\n",
    "    return Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalling X Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller_For_Prediction(Data):\n",
    "    Slot_id=Data['Slot_id']\n",
    "    Key=Data['Key']\n",
    "    Real_Rate=Data['Real_Rate']\n",
    "    \n",
    "    Data=Data.drop(['Slot_id'], axis=1)\n",
    "    Data=Data.drop(['Key'], axis=1)\n",
    "    Data=Data.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Standar_Scaller = pickle.load(open(Project_Path+ '/Standar_Scaller2.pkl','rb'))\n",
    "    Data = Standar_Scaller.transform(Data)\n",
    "    Data=pd.DataFrame(Data,columns =Names)\n",
    "    \n",
    "    Data.insert(0, \"Slot_id\", Slot_id, True)\n",
    "    Data.insert(1, \"Key\", Key, True)\n",
    "    Data.insert(10, \"Real_Rate\", Real_Rate, True)\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller_For_Graph(Train,Test):\n",
    "    Slot_id_Train=Train['Slot_id']\n",
    "    Key_Train=Train['Key']\n",
    "    Real_Rate_Train=Train['Real_Rate']\n",
    "    \n",
    "    Train=Train.drop(['Slot_id'], axis=1)\n",
    "    Train=Train.drop(['Key'], axis=1)\n",
    "    Train=Train.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Slot_id_Test=Test['Slot_id']\n",
    "    Key_Test=Test['Key']\n",
    "    Real_Rate_Test=Test['Real_Rate']\n",
    "    \n",
    "    Test=Test.drop(['Slot_id'], axis=1)\n",
    "    Test=Test.drop(['Key'], axis=1)\n",
    "    Test=Test.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Standar_Scaller = StandardScaler()\n",
    "    Train=Standar_Scaller.fit_transform(Train)\n",
    "    Test = Standar_Scaller.transform(Test)\n",
    "    \n",
    "    Train=pd.DataFrame(Train,columns = Names)\n",
    "    Test=pd.DataFrame(Test,columns = Names)\n",
    "\n",
    "    Train.insert(0, \"Slot_id\", Slot_id_Train, True)\n",
    "    Train.insert(1, \"Key\", Key_Train, True)\n",
    "    Train.insert(10, \"Real_Rate\", Real_Rate_Train, True)\n",
    "    \n",
    "    Test.insert(0, \"Slot_id\", Slot_id_Test, True)\n",
    "    Test.insert(1, \"Key\", Key_Test, True)\n",
    "    Test.insert(10, \"Real_Rate\", Real_Rate_Test, True)\n",
    "    \n",
    "    return Train,Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scalled_Distances(Data):\n",
    "    Distance_Data=Data\n",
    "    Distance_Data=pd.DataFrame(Distance_Data)\n",
    "    Distance_Data=Distance_Data[Names3]\n",
    "    Distance_Data.drop_duplicates(subset =\"Slot_id\",keep ='first', inplace = True)\n",
    "    Distance_Data=Distance_Data.reset_index()\n",
    "    Distance_Data=Distance_Data.drop(['index'], axis=1)\n",
    "    return Distance_Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graphs from Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Unique_Time_Id(Data):\n",
    "    Data=Data.sort_values(by=['Key'])\n",
    "    Time_Id=Data['Key'].unique()\n",
    "    return Time_Id   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delete_Slot_Info(Data):\n",
    "    Data=Data[Names2]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Subsets_and_Lenght(Data,Time_Id):\n",
    "    Subsets=[]\n",
    "    Subsets=[0 for i in range(len(Time_Id))]\n",
    "    Length=[]\n",
    "    Length=[0 for i in range(len(Time_Id))]\n",
    "    Mask=[0 for i in range(len(Time_Id))]\n",
    "\n",
    "    for i in tqdm( range (0,len(Time_Id))):\n",
    "        Subsets[i] = Data[Data[\"Key\"]==Time_Id[i]]\n",
    "        Length[i] = len(Subsets[i])\n",
    "        Mask[i] = Subsets[i]['Slot_id'].values.tolist()\n",
    "    return Subsets,Length,Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan):\n",
    "    Sectors=Legal_Ilegan['Slot_id'].unique()\n",
    "    Sectors=sorted(Sectors)\n",
    "    Datasets=[]\n",
    "    Datasets = [0 for i in range(len(Time_Id))]\n",
    "\n",
    "    for i in tqdm(range (0,len(Subsets))):\n",
    "        S=[]\n",
    "        List=[]\n",
    "        Train_List=Subsets[i].values.tolist()\n",
    "        for j in range (0,Length[i]):\n",
    "            S.append(Train_List[j][0])\n",
    "            List.append(Train_List[j])\n",
    "\n",
    "        for k in range (0,len(Sectors)):\n",
    "            List2=[]\n",
    "            if Sectors[k] not in S:\n",
    "                List2.append(int(Sectors[k]))\n",
    "\n",
    "                for l in range (1,12):\n",
    "                    List2.append(Train_List[0][l])\n",
    "                List.append(List2)\n",
    "\n",
    "        Datasets[i]=pd.DataFrame(List, columns=[Names6])\n",
    "        Datasets[i].Slot_id = Datasets[i].Slot_id.astype(int)\n",
    "    return Datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Distances(Datasets,Time_Id,Distance_Data):\n",
    "    Distance_Data_List=Distance_Data.values.tolist()\n",
    "    for i in tqdm(range(0,len(Time_Id))):\n",
    "        Dataset_List=Datasets[i].values.tolist()\n",
    "        Distances=[]\n",
    "        Distances_Df=[]\n",
    "        for j in range (0,len(Dataset_List)):\n",
    "            for k in range(0,len(Distance_Data_List)):\n",
    "                if Dataset_List[j][0]==Distance_Data_List[k][0]:\n",
    "                    Distances.append(Distance_Data_List[k][1:])\n",
    "                    break\n",
    "        Distances_Df=pd.DataFrame(Distances, columns=[Names4])\n",
    "        Datasets[i] = pd.concat([Datasets[i], Distances_Df], axis=1)\n",
    "\n",
    "    for i in tqdm(range (0,len(Time_Id) )):\n",
    "        Datasets[i]=Datasets[i].drop(['Capacity2'], axis=1)\n",
    "        Datasets[i].insert(5, 'Capacity', Datasets[i].pop('Capacity'))\n",
    "\n",
    "    return Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Using the Regression Model & Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Predictions(Datasets,Time_Id,Length):\n",
    "\n",
    "    Predictions_Df=[]\n",
    "    Predictions_Df=[0 for i in range(len(Datasets))] \n",
    "    Real_Target=[]\n",
    "    Real_Target=[0 for i in range(len(Datasets))] \n",
    "\n",
    "    for i in tqdm(range (0,len(Time_Id))):\n",
    "        Real_Target[i]=np.array(Datasets[i]['Real_Rate'])\n",
    "\n",
    "        Feautures = Datasets[i][Names]\n",
    "        Test=np.array(Feautures)   \n",
    "        Predictions=model.predict([Test])\n",
    "\n",
    "        #Predictions=Inverse_Normalization(Predictions)\n",
    "        Predictions_Df[i]=Predictions\n",
    "\n",
    "    for i in range (0,len(Time_Id)):\n",
    "        for j in range (Length[i],222):\n",
    "            Real_Target[i][j]=Predictions_Df[i][j]\n",
    "    return Real_Target,Predictions_Df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Graph Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Final_Dataset(Datasets,Real_Target,Predictions_Df,Time_Id):\n",
    "    Final_Dataset=[]\n",
    "    Final_Dataset=[0 for i in range(len(Time_Id))] \n",
    "    for i in tqdm(range (0,len(Time_Id))):\n",
    "        Predictions_Df[i]=pd.DataFrame(Predictions_Df[i],columns=['Predictions'])\n",
    "        Real_Target[i]=pd.DataFrame(Real_Target[i],columns=['Target'])\n",
    "        Final_Dataset[i]=pd.concat([Datasets[i][Names5],Predictions_Df[i],Real_Target[i]], axis=1)\n",
    "        Final_Dataset[i].columns=Names7\n",
    "    return Final_Dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cobine all Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Data_For_Predictions(test_data,Distance_Data,Legal_Ilegan):\n",
    "    Time_Id=Get_Unique_Time_Id(test_data)\n",
    "    test_data=Delete_Slot_Info(test_data)\n",
    "    Subsets,Length,Mask=Get_Subsets_and_Lenght(test_data,Time_Id)\n",
    "    Datasets=Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan)\n",
    "    Datasets=Get_Distances(Datasets,Time_Id,Distance_Data)\n",
    "    Real_Target,Predictions_Df=Make_Predictions(Datasets,Time_Id,Length)\n",
    "    return Real_Target,Predictions_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Data(train_data,Distance_Data,Legal_Ilegan,Real_Target,Predictions_Df):\n",
    "    Time_Id=Get_Unique_Time_Id(train_data)\n",
    "    train_data=Delete_Slot_Info(train_data)\n",
    "    Subsets,Length,Mask=Get_Subsets_and_Lenght(train_data,Time_Id)\n",
    "    Datasets=Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan)\n",
    "    Datasets=Get_Distances(Datasets,Time_Id,Distance_Data)    \n",
    "    Train_Dataset=Get_Final_Dataset(Datasets,Real_Target,Predictions_Df,Time_Id)\n",
    "    return Train_Dataset,Length,Mask,Time_Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Test(Test): \n",
    "    Test['Time_Distance']=0\n",
    "    Test=Test.drop(['Time_Int'], axis=1)\n",
    "    a=Test['Slot_Timeint']\n",
    "    b=Test['Ilegality_Rate']\n",
    "    Test=Test.drop(['Slot_Timeint'], axis=1)\n",
    "    Test=Test.drop(['Ilegality_Rate'], axis=1)\n",
    "    Test.insert(9, \"Real_Time\", a, True)\n",
    "    Test.insert(10, \"Real_Rate\", b, True)\n",
    "    \n",
    "    Test=pd.merge(Test, Final_Weather_Data, on='Key')\n",
    "    \n",
    "    Test=pd.merge(Test, Distance_Data, on='Slot_id')\n",
    "    return Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_Data=pd.read_csv(Project_Path+ '/Data/Distance.csv',sep=',',index_col=0)\n",
    "Final_Weather_Data=pd.read_csv(Project_Path+ '/Data/Final_Weather_Data.csv',low_memory=False,sep=',',index_col=0)\n",
    "train_data=pd.read_csv(Project_Path+ '/Data/Train_TimeSeries.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Test_TimeSeries.csv',sep=',',index_col=0)\n",
    "\n",
    "TestDF=Prepare_Test(test_data) #No smoothing\n",
    "TrainDF=Prepare_Test(train_data)\n",
    "\n",
    "TrainDF=TrainDF.drop(['Time_Distance'], axis=1)\n",
    "TestDF=TestDF.drop(['Time_Distance'], axis=1)\n",
    "\n",
    "TrainDF.to_csv(Project_Path+ '/Data/Raw_Train_TimeSeries.csv')\n",
    "TestDF.to_csv(Project_Path+ '/Data/Raw_Test_TimeSeries.csv')\n",
    "\n",
    "TrainDF=TrainDF.drop(['Slot_id'], axis=1)\n",
    "TrainDF=TrainDF.drop(['Key'], axis=1)\n",
    "\n",
    "train_targets=TrainDF['Real_Rate']\n",
    "train_data=TrainDF.drop(['Real_Rate'], axis=1)\n",
    "\n",
    "Standar_Scaller = StandardScaler()\n",
    "train_data=Standar_Scaller.fit_transform(train_data)\n",
    "\n",
    "with open(Project_Path+'/Standar_Scaller2.pkl', 'wb') as f:\n",
    "    pickle.dump(Standar_Scaller, f,  protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(Project_Path+ '/DNN_Regressor')\n",
    "model.compile(optimizer='adamax', loss='MSE', metrics=['MAE'])\n",
    "\n",
    "Legal_Ilegan=pd.read_csv(Project_Path+ '/Data/Full_TimeSeries.csv',sep=',',index_col=0)\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Raw_Test_TimeSeries.csv',sep=',',index_col=0)\n",
    "train_data=pd.read_csv(Project_Path+ '/Data/Raw_Train_TimeSeries.csv',sep=',',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Legal_Ilegan=Prepare_Dataset(Legal_Ilegan)\n",
    "\n",
    "test_data1=Scaller_For_Prediction(test_data)\n",
    "train_data1=Scaller_For_Prediction(train_data)\n",
    "Legal_Ilegan1=Scaller_For_Prediction(Legal_Ilegan)\n",
    "\n",
    "Distance_Data_For_Predictions=Scalled_Distances(Legal_Ilegan1)\n",
    "\n",
    "train_data,test_data=Scaller_For_Graph(train_data,test_data)\n",
    "Distance_Data=Scalled_Distances(Legal_Ilegan1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3702/3702 [00:10<00:00, 346.18it/s]\n",
      "100%|██████████| 3702/3702 [00:05<00:00, 675.61it/s]\n",
      "100%|██████████| 3702/3702 [00:08<00:00, 442.69it/s]\n",
      "  0%|          | 0/3702 [00:00<?, ?it/s]<ipython-input-12-a8167a2d3db1>:16: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  Datasets[i]=Datasets[i].drop(['Capacity2'], axis=1)\n",
      "100%|██████████| 3702/3702 [00:39<00:00, 93.85it/s]\n",
      "100%|██████████| 3702/3702 [01:11<00:00, 51.63it/s]\n",
      "100%|██████████| 3702/3702 [00:10<00:00, 346.86it/s]\n",
      "100%|██████████| 3702/3702 [00:05<00:00, 675.81it/s]\n",
      "100%|██████████| 3702/3702 [00:08<00:00, 441.98it/s]\n",
      "  0%|          | 0/3702 [00:00<?, ?it/s]<ipython-input-12-a8167a2d3db1>:16: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  Datasets[i]=Datasets[i].drop(['Capacity2'], axis=1)\n",
      "100%|██████████| 3702/3702 [00:39<00:00, 94.35it/s]\n",
      "100%|██████████| 3702/3702 [00:02<00:00, 1828.76it/s]\n"
     ]
    }
   ],
   "source": [
    "Real_Target_Train,Predictions_Df_Train=Prepare_Data_For_Predictions(train_data1,Distance_Data_For_Predictions,Legal_Ilegan)\n",
    "Train_Dataset,Length_Train,Train_Mask,Train_Date=Prepare_Data(train_data,Distance_Data,Legal_Ilegan,Real_Target_Train,Predictions_Df_Train)\n",
    "\n",
    "save_object(Train_Dataset, Project_Path+'/Data/Dataset_Graph_For_LP.pkl')\n",
    "with open(Project_Path+'/Data/Length_RawTrain.txt', \"w\") as file:\n",
    "    file.write(str(Length_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2018-01-03 08:00</th>\n",
       "      <th>2018-01-03 09:00</th>\n",
       "      <th>2018-01-03 11:00</th>\n",
       "      <th>2018-01-03 12:00</th>\n",
       "      <th>2018-01-03 14:00</th>\n",
       "      <th>2018-01-03 15:00</th>\n",
       "      <th>2018-01-03 16:00</th>\n",
       "      <th>2018-01-04 08:00</th>\n",
       "      <th>2018-01-04 09:00</th>\n",
       "      <th>2018-01-04 10:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2019-09-29 10:00</th>\n",
       "      <th>2019-09-30 08:00</th>\n",
       "      <th>2019-09-30 09:00</th>\n",
       "      <th>2019-09-30 10:00</th>\n",
       "      <th>2019-09-30 11:00</th>\n",
       "      <th>2019-09-30 12:00</th>\n",
       "      <th>2019-09-30 14:00</th>\n",
       "      <th>2019-09-30 15:00</th>\n",
       "      <th>2019-09-30 16:00</th>\n",
       "      <th>2019-09-30 17:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21502</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28151</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 3702 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2018-01-03 08:00  2018-01-03 09:00  2018-01-03 11:00  2018-01-03 12:00  \\\n",
       "2                     0                 0                 0                 0   \n",
       "3                     0                 0                 0                 0   \n",
       "7                     0                 0                 0                 0   \n",
       "8                     0                 0                 0                 0   \n",
       "9                     0                 0                 0                 0   \n",
       "...                 ...               ...               ...               ...   \n",
       "21453                 0                 0                 0                 0   \n",
       "21502                 0                 0                 0                 0   \n",
       "21591                 0                 0                 0                 0   \n",
       "21612                 0                 0                 0                 0   \n",
       "28151                 0                 0                 0                 0   \n",
       "\n",
       "       2018-01-03 14:00  2018-01-03 15:00  2018-01-03 16:00  2018-01-04 08:00  \\\n",
       "2                     0                 0                 0                 0   \n",
       "3                     0                 0                 0                 0   \n",
       "7                     0                 0                 0                 0   \n",
       "8                     0                 0                 0                 0   \n",
       "9                     0                 1                 0                 1   \n",
       "...                 ...               ...               ...               ...   \n",
       "21453                 0                 0                 0                 0   \n",
       "21502                 0                 0                 0                 0   \n",
       "21591                 0                 0                 0                 0   \n",
       "21612                 0                 0                 0                 0   \n",
       "28151                 0                 0                 0                 0   \n",
       "\n",
       "       2018-01-04 09:00  2018-01-04 10:00  ...  2019-09-29 10:00  \\\n",
       "2                     0                 0  ...                 0   \n",
       "3                     0                 0  ...                 0   \n",
       "7                     0                 0  ...                 0   \n",
       "8                     0                 0  ...                 0   \n",
       "9                     0                 0  ...                 0   \n",
       "...                 ...               ...  ...               ...   \n",
       "21453                 0                 0  ...                 0   \n",
       "21502                 0                 0  ...                 0   \n",
       "21591                 0                 0  ...                 0   \n",
       "21612                 0                 0  ...                 0   \n",
       "28151                 0                 0  ...                 0   \n",
       "\n",
       "       2019-09-30 08:00  2019-09-30 09:00  2019-09-30 10:00  2019-09-30 11:00  \\\n",
       "2                     0                 0                 0                 0   \n",
       "3                     0                 0                 0                 0   \n",
       "7                     0                 0                 0                 0   \n",
       "8                     0                 0                 0                 0   \n",
       "9                     0                 0                 1                 0   \n",
       "...                 ...               ...               ...               ...   \n",
       "21453                 0                 1                 1                 0   \n",
       "21502                 1                 0                 1                 0   \n",
       "21591                 0                 0                 0                 0   \n",
       "21612                 1                 0                 0                 1   \n",
       "28151                 1                 0                 0                 1   \n",
       "\n",
       "       2019-09-30 12:00  2019-09-30 14:00  2019-09-30 15:00  2019-09-30 16:00  \\\n",
       "2                     0                 0                 0                 0   \n",
       "3                     0                 0                 0                 0   \n",
       "7                     0                 0                 0                 0   \n",
       "8                     0                 0                 0                 0   \n",
       "9                     0                 0                 0                 0   \n",
       "...                 ...               ...               ...               ...   \n",
       "21453                 0                 0                 0                 0   \n",
       "21502                 0                 0                 1                 0   \n",
       "21591                 0                 0                 0                 0   \n",
       "21612                 0                 0                 0                 0   \n",
       "28151                 0                 0                 0                 0   \n",
       "\n",
       "       2019-09-30 17:00  \n",
       "2                     0  \n",
       "3                     0  \n",
       "7                     0  \n",
       "8                     0  \n",
       "9                     0  \n",
       "...                 ...  \n",
       "21453                 0  \n",
       "21502                 0  \n",
       "21591                 0  \n",
       "21612                 0  \n",
       "28151                 0  \n",
       "\n",
       "[222 rows x 3702 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Slots=Train_Dataset[0]['Slot_id']\n",
    "Slots=sorted(Slots)\n",
    "\n",
    "Train_Binary_Mask=[]\n",
    "for j in range(0,len(Train_Mask)):\n",
    "    a=[]\n",
    "    for i in range (0,len(Slots)):\n",
    "        a.append(Slots[i] in (Train_Mask[j]))\n",
    "    Train_Binary_Mask.append(a)\n",
    "\n",
    "Train_Binary_Mask=pd.DataFrame(Train_Binary_Mask,columns=Slots,index=Train_Date)\n",
    "Train_Binary_Mask=Train_Binary_Mask.T\n",
    "Train_Binary_Mask = Train_Binary_Mask.replace(False, 0)\n",
    "Train_Binary_Mask = Train_Binary_Mask.replace(True, 1)\n",
    "Train_Binary_Mask.to_csv(Project_Path+ '/Data/Raw_Train_Mask.csv')\n",
    "Train_Binary_Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+'/Data/Dataset_Graph_For_LP.pkl', 'rb') as inp:\n",
    "    Train_Dataset = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:00<00:00, 1847.54it/s]\n"
     ]
    }
   ],
   "source": [
    "Sectors=pd.read_csv(Project_Path+ '/Data/Parking_Slot_Proccesed.csv',sep=',', index_col=0)\n",
    "Nodelist=Sectors[['ΚΩΔΙΚΟΣ ΤΟΜΕΑ','Latitude','Longitude']]\n",
    "a=Nodelist['ΚΩΔΙΚΟΣ ΤΟΜΕΑ']\n",
    "Nodelist=Nodelist.drop(['ΚΩΔΙΚΟΣ ΤΟΜΕΑ'], axis=1)\n",
    "Nodelist.insert(0, \"Slot_id\", a, True)\n",
    "\n",
    "Legal_Ilegan2=pd.read_csv(Project_Path+ '/Data/Scan_Data_Reg_2.3.csv',sep=',',index_col=0)\n",
    "Unique_Sectors=Legal_Ilegan2['Slot_id'].unique()\n",
    "Unique_Sectors=pd.DataFrame(Unique_Sectors, columns=['Slot_id'])\n",
    "Nodelist=pd.merge(Unique_Sectors,Nodelist, on='Slot_id')\n",
    "\n",
    "Nodelist['Slot_id']=sorted(Nodelist['Slot_id'])\n",
    "List=Nodelist.values.tolist()\n",
    "Distance=[]\n",
    "for i in range (0,len(List)):\n",
    "    Distance2=[]\n",
    "    for j in range (0,len(List)):\n",
    "        d= geopy.distance.geodesic((List[i][1],List[i][2]), (List[j][1],List[j][2])).m\n",
    "        Distance2.append(d)\n",
    "    Distance.append(Distance2)\n",
    "Names=Nodelist['Slot_id'].values.tolist()\n",
    "Distance=pd.DataFrame(Distance, columns=Names, index=Names)\n",
    "\n",
    "Columns=Distance.columns\n",
    "NearSectors=[]\n",
    "for i in tqdm(range(0,len(Columns))):\n",
    "    Nearest=Distance.nsmallest(3,Columns[i] )\n",
    "    Nearest=Nearest[Columns[i]]\n",
    "    Nearest = Nearest.reset_index(level=0)\n",
    "    Nearest=Nearest.values.tolist()\n",
    " \n",
    "    if Nearest[1][1]>110:\n",
    "        Nearest[1][1]=110\n",
    "    if Nearest[2][1]>110:\n",
    "        Nearest[2][1]=110\n",
    "    NearSectors.append(Nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Target_Sectros(Train_Dataset,NearSectors):\n",
    "    Sectors_List=[]\n",
    "    for k in tqdm(range (0,len(Train_Dataset))):\n",
    "        Existing_Sectors=Train_Dataset[k]['Slot_id'].values.tolist()\n",
    "        ls=[]\n",
    "        for i in range (0,Length_Train[k]):\n",
    "            ls.append(Existing_Sectors[i])\n",
    "            for j in range (0,len(NearSectors)):\n",
    "                if Existing_Sectors[i]==int(NearSectors[j][0][0]): \n",
    "                    if int(NearSectors[j][1][0]) not in Existing_Sectors[0:Length_Train[k]]:\n",
    "                        ls.append(int(NearSectors[j][1][0]))\n",
    "                    if NearSectors[j][2][0] not in Existing_Sectors[0:Length_Train[k]]:\n",
    "                        ls.append(int(NearSectors[j][2][0]))\n",
    "        x= np.array(ls)\n",
    "        ls=np.unique(x)       \n",
    "        Sectors_List.append(ls)\n",
    "    return Sectors_List\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3702/3702 [00:02<00:00, 1702.26it/s]\n"
     ]
    }
   ],
   "source": [
    "Sectors_List=Get_Target_Sectros(Train_Dataset,NearSectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3702/3702 [00:01<00:00, 2498.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range (0,len(Train_Dataset))):\n",
    "    T=[]\n",
    "    ls=Train_Dataset[i]['Slot_id'].values.tolist()\n",
    "    for j in range (0,len(ls)):\n",
    "        if ls[j] not in Sectors_List[i]:\n",
    "            T.append(False)\n",
    "        else:\n",
    "            T.append(True)\n",
    "    Train_Dataset[i]['Flag']=T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(Train_Dataset, Project_Path+ '/Data/Dataset_Graph_For_LP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-aa0d996297cc>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TrainDF['Real_Rate']=round(TrainDF['Real_Rate'],2)\n",
      "<ipython-input-30-aa0d996297cc>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TrainDF['Real_Rate']=TrainDF['Real_Rate']*100\n",
      "<ipython-input-30-aa0d996297cc>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TrainDF['Real_Rate']=TrainDF['Real_Rate'].astype(int)\n",
      "<ipython-input-30-aa0d996297cc>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TrainDF['Real_Rate'].mask(TrainDF['Real_Rate'] == -100, -1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "with open(Project_Path+'/Data/Dataset_Graph_For_LP.pkl', 'rb') as inp:\n",
    "    Train_Dataset = pickle.load(inp)\n",
    "\n",
    "for i in range (0,len(Train_Dataset)):\n",
    "    Train_Dataset[i]['Target'].mask(Train_Dataset[i]['Target'] == Train_Dataset[i]['Predictions'], -1, inplace=True)\n",
    "    Train_Dataset[i]=Train_Dataset[i].loc[Train_Dataset[i]['Flag'] == True]\n",
    "    \n",
    "Dataset = pd.concat(Train_Dataset, axis=0)\n",
    "Dataset['Real_Rate']=Dataset['Target']\n",
    "TrainDF=Dataset[Names_LP]\n",
    "\n",
    "TrainDF['Real_Rate']=round(TrainDF['Real_Rate'],2)\n",
    "TrainDF['Real_Rate']=TrainDF['Real_Rate']*100\n",
    "TrainDF['Real_Rate']=TrainDF['Real_Rate'].astype(int)\n",
    "TrainDF['Real_Rate'].mask(TrainDF['Real_Rate'] == -100, -1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=TrainDF\n",
    "y=Data['Real_Rate']\n",
    "X=Data.drop(['Real_Rate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rates=Data['Real_Rate'].unique()\n",
    "Rates=Rates/100\n",
    "Rates=sorted(Rates)\n",
    "Rates=Rates[1:]\n",
    "len(Rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelPropagation(kernel=&#x27;knn&#x27;, n_jobs=-1, n_neighbors=301)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelPropagation</label><div class=\"sk-toggleable__content\"><pre>LabelPropagation(kernel=&#x27;knn&#x27;, n_jobs=-1, n_neighbors=301)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelPropagation(kernel='knn', n_jobs=-1, n_neighbors=301)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LabelPropagation(kernel='knn',n_neighbors=301, max_iter=1000,n_jobs=-1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Project_Path +'/LabelPropagation.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.7",
   "language": "python",
   "name": "tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
