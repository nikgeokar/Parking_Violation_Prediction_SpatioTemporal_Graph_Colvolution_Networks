{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Graph Temporal Signal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import geopy.distance\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "Project_Path='Project/Path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Names=['Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','temp','humidity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι','Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names2=['Slot_id','Key','Date_Sin','Covid','Holidays','Capacity','Week_Day_Sin','Month_Sin','Real_Time','Real_Rate','temp','humidity']\n",
    "Names3=['Slot_id','Capacity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names4=['Capacity','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names5=['Slot_id','Covid','Holidays','Capacity','temp','humidity','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης']\n",
    "Names6=['Slot_id','Key','Date_Sin','Covid','Holidays','Capacity2','Week_Day_Sin','Month_Sin','Real_Time','Real_Rate','temp','humidity']\n",
    "Names7=['Slot_id','Covid','Holidays','Capacity','temp','humidity','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Predictions','Target']\n",
    "Names9=['Slot_id','Date_Sin','Week_Day_Sin','Month_Sin','Real_Time','Γενικό Νοσοκομείο Θεσσαλονίκης «Γ. Γεννηματάς»', 'Λιμάνι' ,'Δημαρχείο Θεσσαλονίκης','Λευκός Πύργος','Αγορά Καπάνι','Λαδάδικα','Πλατεία Άθωνος','Πλατεία Αριστοτέλους','Ροτόντα','Πλατεία Αγίας Σοφίας','Πλατεία Αντιγονιδών','Μουσείο Μακεδονικού Αγώνα','Πλατεία Ναυαρίνου','Πάρκο ΧΑΝΘ','Ιερός Ναός Αγίου Δημητρίου','ΔΕΘ','ΑΠΘ','Άγαλμα Ελευθερίου Βενιζέλου','Ρωμαϊκή Αγορά Θεσσαλονίκης','Predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Dataset(Test): \n",
    "    Test=Test.drop(['Time_Int'], axis=1)\n",
    "    a=Test['Slot_Timeint']\n",
    "    b=Test['Ilegality_Rate']\n",
    "    Test=Test.drop(['Slot_Timeint'], axis=1)\n",
    "    Test=Test.drop(['Ilegality_Rate'], axis=1)\n",
    "    Test.insert(8, \"Real_Time\", a, True)\n",
    "    Test.insert(9, \"Real_Rate\", b, True)\n",
    "    \n",
    "    Final_Weather_Data=pd.read_csv(Project_Path+ '/Data/Final_Weather_Data.csv',low_memory=False,sep=',',index_col=0)\n",
    "    Test=pd.merge(Test, Final_Weather_Data, on='Key')\n",
    "    Distance_Df=pd.read_csv(Project_Path+ '/Data/Distance.csv',sep=',',index_col=0)\n",
    "    Test=pd.merge(Test, Distance_Df, on='Slot_id')\n",
    "    return Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalling X Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller_For_Prediction(Data):\n",
    "    Slot_id=Data['Slot_id']\n",
    "    Key=Data['Key']\n",
    "    Real_Rate=Data['Real_Rate']\n",
    "    \n",
    "    Data=Data.drop(['Slot_id'], axis=1)\n",
    "    Data=Data.drop(['Key'], axis=1)\n",
    "    Data=Data.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Standar_Scaller = pickle.load(open(Project_Path+ '/Standar_Scaller.pkl','rb'))\n",
    "    Data = Standar_Scaller.transform(Data)\n",
    "    Data=pd.DataFrame(Data,columns =Names)\n",
    "    \n",
    "    Data.insert(0, \"Slot_id\", Slot_id, True)\n",
    "    Data.insert(1, \"Key\", Key, True)\n",
    "    Data.insert(10, \"Real_Rate\", Real_Rate, True)\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaller_For_Graph(Train,Test):\n",
    "    Slot_id_Train=Train['Slot_id']\n",
    "    Key_Train=Train['Key']\n",
    "    Real_Rate_Train=Train['Real_Rate']\n",
    "    \n",
    "    Train=Train.drop(['Slot_id'], axis=1)\n",
    "    Train=Train.drop(['Key'], axis=1)\n",
    "    Train=Train.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Slot_id_Test=Test['Slot_id']\n",
    "    Key_Test=Test['Key']\n",
    "    Real_Rate_Test=Test['Real_Rate']\n",
    "    \n",
    "    Test=Test.drop(['Slot_id'], axis=1)\n",
    "    Test=Test.drop(['Key'], axis=1)\n",
    "    Test=Test.drop(['Real_Rate'], axis=1)\n",
    "    \n",
    "    Standar_Scaller = StandardScaler()\n",
    "    Train=Standar_Scaller.fit_transform(Train)\n",
    "    Test = Standar_Scaller.transform(Test)\n",
    "    \n",
    "    Train=pd.DataFrame(Train,columns = Names)\n",
    "    Test=pd.DataFrame(Test,columns = Names)\n",
    "\n",
    "    Train.insert(0, \"Slot_id\", Slot_id_Train, True)\n",
    "    Train.insert(1, \"Key\", Key_Train, True)\n",
    "    Train.insert(10, \"Real_Rate\", Real_Rate_Train, True)\n",
    "    \n",
    "    Test.insert(0, \"Slot_id\", Slot_id_Test, True)\n",
    "    Test.insert(1, \"Key\", Key_Test, True)\n",
    "    Test.insert(10, \"Real_Rate\", Real_Rate_Test, True)\n",
    "    \n",
    "    return Train,Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scalled_Distances(Data):\n",
    "    Distance_Data=Data\n",
    "    Distance_Data=pd.DataFrame(Distance_Data)\n",
    "    Distance_Data=Distance_Data[Names3]\n",
    "    Distance_Data.drop_duplicates(subset =\"Slot_id\",keep ='first', inplace = True)\n",
    "    Distance_Data=Distance_Data.reset_index()\n",
    "    Distance_Data=Distance_Data.drop(['index'], axis=1)\n",
    "    return Distance_Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Label Propagation distrubtion output to single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_Distrubtion(Proba):\n",
    "    Classes=[]\n",
    "    Exclude=[0.01,0.29, 0.58, 0.99]\n",
    "    Predictions=[]\n",
    "    \n",
    "    for i in range (0,101):\n",
    "        Class=i/100\n",
    "        if Class not in Exclude:\n",
    "            Classes.append(Class)\n",
    "    \n",
    "    for j in range (0,len(Proba)):\n",
    "        Prediction=0\n",
    "        for i in range(0,len(Proba[j])):\n",
    "            if Proba[j][i]>0:\n",
    "                Prediction=Prediction+Proba[j][i]*Classes[i]\n",
    "        Predictions.append(Prediction)\n",
    "    \n",
    "    return Predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph signal from initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Unique_Time_Id(Data):\n",
    "    Data=Data.sort_values(by=['Key'])\n",
    "    Time_Id=Data['Key'].unique()\n",
    "    return Time_Id   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delete_Slot_Info(Data):\n",
    "    Data=Data[Names2]\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Subsets_and_Lenght(Data,Time_Id):\n",
    "    Subsets=[]\n",
    "    Subsets=[0 for i in range(len(Time_Id))]\n",
    "    Length=[]\n",
    "    Length=[0 for i in range(len(Time_Id))]\n",
    "    Mask=[0 for i in range(len(Time_Id))]\n",
    "\n",
    "    for i in tqdm( range (0,len(Time_Id))):\n",
    "        Subsets[i] = Data[Data[\"Key\"]==Time_Id[i]]\n",
    "        Length[i] = len(Subsets[i])\n",
    "        Mask[i] = Subsets[i]['Slot_id'].values.tolist()\n",
    "    return Subsets,Length,Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan):\n",
    "    Sectors=Legal_Ilegan['Slot_id'].unique()\n",
    "    Sectors=sorted(Sectors)\n",
    "    Datasets=[]\n",
    "    Datasets = [0 for i in range(len(Time_Id))]\n",
    "\n",
    "    for i in tqdm(range (0,len(Subsets))):\n",
    "        S=[]\n",
    "        List=[]\n",
    "        Train_List=Subsets[i].values.tolist()\n",
    "        for j in range (0,Length[i]):\n",
    "            S.append(Train_List[j][0])\n",
    "            List.append(Train_List[j])\n",
    "\n",
    "        for k in range (0,len(Sectors)):\n",
    "            List2=[]\n",
    "            if Sectors[k] not in S:\n",
    "                List2.append(int(Sectors[k]))\n",
    "\n",
    "                for l in range (1,12):\n",
    "                    List2.append(Train_List[0][l])\n",
    "                List.append(List2)\n",
    "\n",
    "        Datasets[i]=pd.DataFrame(List, columns=[Names6])\n",
    "        Datasets[i].Slot_id = Datasets[i].Slot_id.astype(int)\n",
    "    return Datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Distances(Datasets,Time_Id,Distance_Data):\n",
    "    Distance_Data_List=Distance_Data.values.tolist()\n",
    "    for i in tqdm(range(0,len(Time_Id))):\n",
    "        Dataset_List=Datasets[i].values.tolist()\n",
    "        Distances=[]\n",
    "        Distances_Df=[]\n",
    "        for j in range (0,len(Dataset_List)):\n",
    "            for k in range(0,len(Distance_Data_List)):\n",
    "                if Dataset_List[j][0]==Distance_Data_List[k][0]:\n",
    "                    Distances.append(Distance_Data_List[k][1:])\n",
    "                    break\n",
    "        Distances_Df=pd.DataFrame(Distances, columns=[Names4])\n",
    "        Datasets[i] = pd.concat([Datasets[i], Distances_Df], axis=1)\n",
    "\n",
    "    for i in tqdm(range (0,len(Time_Id) )):\n",
    "        Datasets[i]=Datasets[i].drop(['Capacity2'], axis=1)\n",
    "        Datasets[i].insert(5, 'Capacity', Datasets[i].pop('Capacity'))\n",
    "\n",
    "    return Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the DNN & Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Predictions(Datasets,Time_Id,Length):\n",
    "\n",
    "    Predictions_Df=[]\n",
    "    Predictions_Df=[0 for i in range(len(Datasets))] \n",
    "    Real_Target=[]\n",
    "    Real_Target=[0 for i in range(len(Datasets))] \n",
    "    Labels=[]\n",
    "    Labels=[0 for i in range(len(Datasets))]\n",
    "\n",
    "    for i in tqdm(range (0,len(Time_Id))):\n",
    "        Real_Target[i]=np.array(Datasets[i]['Real_Rate'])\n",
    "\n",
    "        Feautures = Datasets[i][Names]\n",
    "        Test=np.array(Feautures)   \n",
    "        Predictions=model.predict([Test])\n",
    "        \n",
    "        Predictions_Df[i]=Predictions\n",
    "        \n",
    "        Proba=Label_Propagation.predict_proba(Feautures)\n",
    "        LP_Predictions=Calculate_Distrubtion(Proba)\n",
    "        Labels[i]=LP_Predictions\n",
    "\n",
    "    for i in range (0,len(Time_Id)):\n",
    "        for j in range (Length[i],222):\n",
    "            Real_Target[i][j]=Labels[i][j]\n",
    "    return Real_Target,Predictions_Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Final_Dataset(Datasets,Real_Target,Predictions_Df,Time_Id):\n",
    "    Final_Dataset=[]\n",
    "    Final_Dataset=[0 for i in range(len(Time_Id))] \n",
    "    for i in tqdm(range (0,len(Time_Id))):\n",
    "        Predictions_Df[i]=pd.DataFrame(Predictions_Df[i],columns=['Predictions'])\n",
    "        Real_Target[i]=pd.DataFrame(Real_Target[i],columns=['Target'])\n",
    "        Final_Dataset[i]=pd.concat([Datasets[i][Names5],Predictions_Df[i],Real_Target[i]], axis=1)\n",
    "        Final_Dataset[i].columns=Names7\n",
    "    return Final_Dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Data_For_Predictions(test_data,Distance_Data,Legal_Ilegan):\n",
    "    Time_Id=Get_Unique_Time_Id(test_data)\n",
    "    test_data=Delete_Slot_Info(test_data)\n",
    "    Subsets,Length,Mask=Get_Subsets_and_Lenght(test_data,Time_Id)\n",
    "    Datasets=Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan)\n",
    "    Datasets=Get_Distances(Datasets,Time_Id,Distance_Data)\n",
    "    Real_Target,Predictions_Df=Make_Predictions(Datasets,Time_Id,Length)\n",
    "    return Real_Target,Predictions_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Data(train_data,Distance_Data,Legal_Ilegan,Real_Target,Predictions_Df):\n",
    "    Time_Id=Get_Unique_Time_Id(train_data)\n",
    "    train_data=Delete_Slot_Info(train_data)\n",
    "    Subsets,Length,Mask=Get_Subsets_and_Lenght(train_data,Time_Id)\n",
    "    Datasets=Fill_Subsets(Subsets,Time_Id,Length,Legal_Ilegan)\n",
    "    Datasets=Get_Distances(Datasets,Time_Id,Distance_Data)    \n",
    "    Train_Dataset=Get_Final_Dataset(Datasets,Real_Target,Predictions_Df,Time_Id)\n",
    "    return Train_Dataset,Length,Mask,Time_Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and initial preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(Project_Path+ '/DNN_Regressor')\n",
    "model.compile(optimizer='adamax', loss='MSE', metrics=['MAE'])\n",
    "Label_Propagation = pickle.load(open(Project_Path+'/LabelPropagation.sav', 'rb'))\n",
    "\n",
    "Legal_Ilegan=pd.read_csv(Project_Path+ '/Data/Full_TimeSeries.csv',sep=',',index_col=0)\n",
    "\n",
    "test_data=pd.read_csv(Project_Path+ '/Data/Ts_Test_TimeSeries.csv',sep=',',index_col=0)\n",
    "\n",
    "train_data=pd.read_csv(Project_Path+ '/Data/Ts_Train_TimeSeries.csv',sep=',',index_col=0)\n",
    "\n",
    "Legal_Ilegan=Prepare_Dataset(Legal_Ilegan)\n",
    "\n",
    "test_data1=Scaller_For_Prediction(test_data)\n",
    "train_data1=Scaller_For_Prediction(train_data)\n",
    "Legal_Ilegan1=Scaller_For_Prediction(Legal_Ilegan)\n",
    "\n",
    "Distance_Data_For_Predictions=Scalled_Distances(Legal_Ilegan1)\n",
    "\n",
    "train_data,test_data=Scaller_For_Graph(train_data,test_data)\n",
    "Distance_Data=Scalled_Distances(Legal_Ilegan1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph-based trainset & testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real_Target_Train,Predictions_Df_Train=Prepare_Data_For_Predictions(train_data1,Distance_Data_For_Predictions,Legal_Ilegan)\n",
    "Train_Dataset,Length_Train,Train_Mask,Train_Date=Prepare_Data(train_data,Distance_Data,Legal_Ilegan,Real_Target_Train,Predictions_Df_Train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real_Target_Test,Predictions_Df_Test=Prepare_Data_For_Predictions(test_data1,Distance_Data_For_Predictions,Legal_Ilegan)\n",
    "Test_Dataset,Length_Test,Test_Mask,Test_Date=Prepare_Data(test_data,Distance_Data,Legal_Ilegan,Real_Target_Test,Predictions_Df_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save graph objects and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(Train_Dataset, Project_Path+'/Data/Dataset_Graph.pkl')\n",
    "save_object(Test_Dataset, Project_Path+'/Data/Test_Dataset_Graph.pkl')\n",
    "\n",
    "with open(Project_Path+'/Data/Length_Train.txt', \"w\") as file:\n",
    "    file.write(str(Length_Train))\n",
    "with open(Project_Path+'/Data/Length_Test.txt', \"w\") as file:\n",
    "    file.write(str(Length_Test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Target Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graph object and mask Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+'/Data/Dataset_Graph.pkl', 'rb') as inp:\n",
    "    Train_Dataset = pickle.load(inp)\n",
    "with open(Project_Path+'/Data/Test_Dataset_Graph.pkl', 'rb') as inp:\n",
    "    Test_Dataset = pickle.load(inp)\n",
    "with open(Project_Path+'/Data/Length_Train.txt', \"r\") as file:\n",
    "    Length_Train = eval(file.readline())\n",
    "with open(Project_Path+'/Data/Length_Test.txt', \"r\") as file:\n",
    "    Length_Test = eval(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train target time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(Train_Dataset)):\n",
    "    Name=Train_Date[i]\n",
    "    Train_Dataset[i].rename(columns = {'Target':Name}, inplace = True)\n",
    "    Train_Dataset[i]=Train_Dataset[i].sort_values(\"Slot_id\")\n",
    "    Train_Dataset[i]=Train_Dataset[i].set_index('Slot_id')\n",
    "    Train_Dataset[i]=Train_Dataset[i][Name]\n",
    "    \n",
    "speeds_array=Train_Dataset[0]\n",
    "for i in tqdm(range (1,len(Train_Dataset))):\n",
    "    speeds_array=pd.merge(speeds_array,Train_Dataset[i],left_index=True, right_index=True)\n",
    "    \n",
    "speeds_array.to_csv(Project_Path+ '/Data/Rate_Timeseries.csv')\n",
    "speeds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test target time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(Test_Dataset)):\n",
    "    Name=Test_Date[i]\n",
    "    Test_Dataset[i].rename(columns = {'Target':Name}, inplace = True)\n",
    "    Test_Dataset[i]=Test_Dataset[i].sort_values(\"Slot_id\")\n",
    "    Test_Dataset[i]=Test_Dataset[i].set_index('Slot_id')\n",
    "    Test_Dataset[i]=Test_Dataset[i][Name]\n",
    "speeds_array=Test_Dataset[0]\n",
    "for i in tqdm(range (1,len(Test_Dataset))):\n",
    "    speeds_array=pd.merge(speeds_array,Test_Dataset[i],left_index=True, right_index=True)\n",
    "    \n",
    "speeds_array.to_csv(Project_Path+ '/Data/Test_Rate_Timeseries.csv')\n",
    "speeds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Data/Test_Dataset_Graph.pkl', 'rb') as inp:\n",
    "    Test_Dataset = pickle.load(inp)\n",
    "    \n",
    "Slots=Test_Dataset[0]['Slot_id']\n",
    "Slots=sorted(Slots)\n",
    "\n",
    "Test_Binary_Mask=[]\n",
    "for j in range(0,len(Test_Mask)):\n",
    "    a=[]\n",
    "    for i in range (0,len(Slots)):\n",
    "        a.append(Slots[i] in (Test_Mask[j]))\n",
    "    Test_Binary_Mask.append(a)\n",
    "\n",
    "Test_Binary_Mask=pd.DataFrame(Test_Binary_Mask,columns=Slots,index=Test_Date)\n",
    "Test_Binary_Mask=Test_Binary_Mask.T\n",
    "Test_Binary_Mask = Test_Binary_Mask.replace(False, 0)\n",
    "Test_Binary_Mask = Test_Binary_Mask.replace(True, 1)\n",
    "Test_Binary_Mask.to_csv(Project_Path+ '/Data/Test_Mask.csv')\n",
    "Test_Binary_Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Data/Dataset_Graph.pkl', 'rb') as inp:\n",
    "    Train_Dataset = pickle.load(inp)\n",
    "    \n",
    "Slots=Train_Dataset[0]['Slot_id']\n",
    "Slots=sorted(Slots)\n",
    "\n",
    "Train_Binary_Mask=[]\n",
    "for j in range(0,len(Train_Mask)):\n",
    "    a=[]\n",
    "    for i in range (0,len(Slots)):\n",
    "        a.append(Slots[i] in (Train_Mask[j]))\n",
    "    Train_Binary_Mask.append(a)\n",
    "\n",
    "Train_Binary_Mask=pd.DataFrame(Train_Binary_Mask,columns=Slots,index=Train_Date)\n",
    "Train_Binary_Mask=Train_Binary_Mask.T\n",
    "Train_Binary_Mask = Train_Binary_Mask.replace(False, 0)\n",
    "Train_Binary_Mask = Train_Binary_Mask.replace(True, 1)\n",
    "Train_Binary_Mask.to_csv(Project_Path+ '/Data/Train_Mask.csv')\n",
    "Train_Binary_Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Label Propagation technique only on neighbor nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Project_Path+ '/Data/Dataset_Graph.pkl', 'rb') as inp:\n",
    "    Train_Dataset = pickle.load(inp)\n",
    "\n",
    "with open(Project_Path+ '/Data/Length_Train.txt', \"r\") as file:\n",
    "    Length_Train = eval(file.readline())\n",
    "    \n",
    "Target=pd.read_csv(Project_Path+ '/Data/Rate_Timeseries.csv',sep=',', index_col=0)\n",
    "Train_Date=Target.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find nearest unlabeled nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sectors=pd.read_csv(Project_Path+ '/Data/Parking_Slot_Proccesed.csv',sep=',', index_col=0)\n",
    "Nodelist=Sectors[['ΚΩΔΙΚΟΣ ΤΟΜΕΑ','Latitude','Longitude']]\n",
    "a=Nodelist['ΚΩΔΙΚΟΣ ΤΟΜΕΑ']\n",
    "Nodelist=Nodelist.drop(['ΚΩΔΙΚΟΣ ΤΟΜΕΑ'], axis=1)\n",
    "Nodelist.insert(0, \"Slot_id\", a, True)\n",
    "\n",
    "Legal_Ilegan2=pd.read_csv(Project_Path+ '/Data/Scan_Data_Reg_2.3.csv',sep=',',index_col=0)\n",
    "Unique_Sectors=Legal_Ilegan2['Slot_id'].unique()\n",
    "Unique_Sectors=pd.DataFrame(Unique_Sectors, columns=['Slot_id'])\n",
    "Nodelist=pd.merge(Unique_Sectors,Nodelist, on='Slot_id')\n",
    "\n",
    "Nodelist['Slot_id']=sorted(Nodelist['Slot_id'])\n",
    "List=Nodelist.values.tolist()\n",
    "Distance=[]\n",
    "for i in range (0,len(List)):\n",
    "    Distance2=[]\n",
    "    for j in range (0,len(List)):\n",
    "        d= geopy.distance.geodesic((List[i][1],List[i][2]), (List[j][1],List[j][2])).m\n",
    "        Distance2.append(d)\n",
    "    Distance.append(Distance2)\n",
    "Names=Nodelist['Slot_id'].values.tolist()\n",
    "Distance=pd.DataFrame(Distance, columns=Names, index=Names)\n",
    "\n",
    "Columns=Distance.columns\n",
    "NearSectors=[]\n",
    "for i in tqdm(range(0,len(Columns))):\n",
    "    Nearest=Distance.nsmallest(3,Columns[i] )\n",
    "    Nearest=Nearest[Columns[i]]\n",
    "    Nearest = Nearest.reset_index(level=0)\n",
    "    Nearest=Nearest.values.tolist()\n",
    "    NearSectors.append(Nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Target_Sectros(Train_Dataset,NearSectors):\n",
    "    Sectors_List=[]\n",
    "    for k in tqdm(range (0,len(Train_Dataset))):\n",
    "        Existing_Sectors=Train_Dataset[k]['Slot_id'].values.tolist()\n",
    "        ls=[]\n",
    "        for i in range (0,Length_Train[k]):\n",
    "            ls.append(Existing_Sectors[i])\n",
    "            for j in range (0,len(NearSectors)):\n",
    "                if Existing_Sectors[i]==int(NearSectors[j][0][0]): \n",
    "                    if int(NearSectors[j][1][0]) not in Existing_Sectors[0:Length_Train[k]]:\n",
    "                        ls.append(int(NearSectors[j][1][0]))\n",
    "                    if NearSectors[j][2][0] not in Existing_Sectors[0:Length_Train[k]]:\n",
    "                        ls.append(int(NearSectors[j][2][0]))\n",
    "        x= np.array(ls)\n",
    "        ls=np.unique(x)       \n",
    "        Sectors_List.append(ls)\n",
    "    return Sectors_List         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sectors_List=Get_Target_Sectros(Train_Dataset,NearSectors)\n",
    "\n",
    "for i in tqdm(range (0,len(Train_Dataset))):\n",
    "    T=[]\n",
    "    ls=Train_Dataset[i]['Slot_id'].values.tolist()\n",
    "    for j in range (0,len(ls)):\n",
    "        if ls[j] not in Sectors_List[i]:\n",
    "            T.append(False)\n",
    "        else:\n",
    "            T.append(True)\n",
    "    Train_Dataset[i]['Flag']=T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep Label Propagation predictions only for the neighbor nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(Train_Dataset)):\n",
    "    Train_Dataset[i]['Target'].mask(Train_Dataset[i]['Flag'] == False, Train_Dataset[i]['Predictions'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(Train_Dataset)):\n",
    "    Name=Train_Date[i]\n",
    "    Train_Dataset[i].rename(columns = {'Target':Name}, inplace = True)\n",
    "    Train_Dataset[i]=Train_Dataset[i].sort_values(\"Slot_id\")\n",
    "    Train_Dataset[i]=Train_Dataset[i].set_index('Slot_id')\n",
    "    Train_Dataset[i]=Train_Dataset[i][Name]\n",
    "    \n",
    "speeds_array=Train_Dataset[0]\n",
    "for i in tqdm(range (1,len(Train_Dataset))):\n",
    "    speeds_array=pd.merge(speeds_array,Train_Dataset[i],left_index=True, right_index=True)\n",
    "    \n",
    "speeds_array.to_csv(Project_Path+ '/Data/STS_Rate_Timeseries.csv')\n",
    "speeds_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.7",
   "language": "python",
   "name": "tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
